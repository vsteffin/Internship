{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee296716",
   "metadata": {},
   "source": [
    "### 1. Python Program to scrape data for 'Data Analyst' Job Poisition in 'Bangalore' Location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b7b5c",
   "metadata": {},
   "source": [
    "Website = 'http://www.naukri.com/'\n",
    "\n",
    "Details to be scraped\n",
    "\n",
    "    1. job-title\n",
    "    2. job-location\n",
    "    3. company_name\n",
    "    4. experience required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369af3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fff650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver \n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362a97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f39f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “Data Analyst” in “Skill, Designations, Companies” field.\n",
    "\n",
    "search_field_desgination = driver.find_element_by_class_name('suggestor-input ')\n",
    "search_field_desgination.send_keys('Data Analyst')\n",
    "\n",
    "#Entering \"Bangalore” in “enter the location” field.\n",
    "\n",
    "search_field_location = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_field_location.send_keys('Bangalore')\n",
    "\n",
    "#Clicking the search button\n",
    "\n",
    "search_button = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e88787",
   "metadata": {},
   "source": [
    "#### Extracting  job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605e7b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Data Analyst (Digital Services Analytics)',\n",
       " 'Senior Data Analyst',\n",
       " 'Sr Domain Expert -Data Analysts',\n",
       " 'Data Analyst',\n",
       " 'Senior Data Analyst',\n",
       " 'Senior Data Analyst - Collibra',\n",
       " 'Data Analyst - IIM/ISB/MDI/FMS/SP Jain',\n",
       " 'Data Analyst',\n",
       " 'Senior Professional Data Analyst',\n",
       " 'Data Analyst - Python/Artificial Intelligence']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title = []\n",
    "\n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "for t in title_tags:\n",
    "    title = t.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "job_title = job_title[:10]\n",
    "job_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9882a5",
   "metadata": {},
   "source": [
    "#### Extracting job location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce43f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secunderabad, Pune, Chennai, Ahmedabad, Delhi / NCR, Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location = []\n",
    "\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "\n",
    "for loc in location_tags:\n",
    "    location = loc.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "job_location = job_location[:10]\n",
    "job_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0450d6",
   "metadata": {},
   "source": [
    "#### Extracting company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73d0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dell Technologies',\n",
       " 'Cerner',\n",
       " 'Siemens',\n",
       " 'Enrich talents',\n",
       " 'Intel',\n",
       " 'Shell',\n",
       " 'K12 Techno Services Pvt Ltd',\n",
       " 'Vedantu Innovations',\n",
       " 'DXC Technology',\n",
       " 'iMindYourBusiness']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_names = []\n",
    "\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "for com in company_tags:\n",
    "    company = com.text\n",
    "    company_names.append(company)\n",
    "company_names = company_names[:10]\n",
    "\n",
    "company_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683addc9",
   "metadata": {},
   "source": [
    "#### Extracting experience required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a918b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-6 Yrs',\n",
       " '8-12 Yrs',\n",
       " '0-10 Yrs',\n",
       " '0-4 Yrs',\n",
       " '3-8 Yrs',\n",
       " '8-12 Yrs',\n",
       " '4-9 Yrs',\n",
       " '0-3 Yrs',\n",
       " '3-7 Yrs',\n",
       " '0-2 Yrs']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_required = []\n",
    "\n",
    "experience_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "\n",
    "for exp in experience_tags:\n",
    "    experience = exp.text\n",
    "    experience_required.append(experience)\n",
    "    \n",
    "experience_required = experience_required[:10]\n",
    "experience_required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c23ef",
   "metadata": {},
   "source": [
    "Now we have scraped the required data from the website. We can check the length of elements in each list to create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bb4225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_names),len(experience_required))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7ff20",
   "metadata": {},
   "source": [
    "#### Closing the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95974275",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27dfe66",
   "metadata": {},
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658aee06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst (Digital Services Analytics)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Dell Technologies</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cerner</td>\n",
       "      <td>8-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sr Domain Expert -Data Analysts</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>0-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Enrich talents</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Intel</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst - Collibra</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell</td>\n",
       "      <td>8-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - IIM/ISB/MDI/FMS/SP Jain</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>K12 Techno Services Pvt Ltd</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Vedantu Innovations</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Professional Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "      <td>Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0  Senior Data Analyst (Digital Services Analytics)   \n",
       "1                               Senior Data Analyst   \n",
       "2                   Sr Domain Expert -Data Analysts   \n",
       "3                                      Data Analyst   \n",
       "4                               Senior Data Analyst   \n",
       "5                    Senior Data Analyst - Collibra   \n",
       "6            Data Analyst - IIM/ISB/MDI/FMS/SP Jain   \n",
       "7                                      Data Analyst   \n",
       "8                  Senior Professional Data Analyst   \n",
       "9     Data Analyst - Python/Artificial Intelligence   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...   \n",
       "\n",
       "                       Company Experience Required  \n",
       "0            Dell Technologies             1-6 Yrs  \n",
       "1                       Cerner            8-12 Yrs  \n",
       "2                      Siemens            0-10 Yrs  \n",
       "3               Enrich talents             0-4 Yrs  \n",
       "4                        Intel             3-8 Yrs  \n",
       "5                        Shell            8-12 Yrs  \n",
       "6  K12 Techno Services Pvt Ltd             4-9 Yrs  \n",
       "7          Vedantu Innovations             0-3 Yrs  \n",
       "8               DXC Technology             3-7 Yrs  \n",
       "9            iMindYourBusiness             0-2 Yrs  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = pd.DataFrame({})\n",
    "\n",
    "jobs['Title'] = job_title\n",
    "jobs['Location'] = job_location\n",
    "jobs['Company'] = company_names\n",
    "jobs['Experience Required'] = experience_required\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393c3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300105a8",
   "metadata": {},
   "source": [
    "### 2. Python Program to scrape data for 'Data Scientist' Job Poisition in 'Bangalore' Location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b8ac7",
   "metadata": {},
   "source": [
    "Website = 'http://www.naukri.com/'\n",
    "\n",
    "Details to be scraped\n",
    "\n",
    "    1. job-title\n",
    "    2. job-location\n",
    "    3. company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fd54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ef5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver \n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f46304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b518a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “Data Scientist” in “Skill, Designations, Companies” field.\n",
    "\n",
    "search_field_desgination = driver.find_element_by_class_name('suggestor-input ')\n",
    "search_field_desgination.send_keys('Data Scientist')\n",
    "\n",
    "#Entering \"Bangalore” in “enter the location” field.\n",
    "\n",
    "search_field_location = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_field_location.send_keys('Bangalore')\n",
    "\n",
    "#Clicking the search button\n",
    "\n",
    "search_button = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362308c",
   "metadata": {},
   "source": [
    "#### Extracting  job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8d632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Junior Data Scientist',\n",
       " 'Staff Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist II- Merchandise & Discovery',\n",
       " 'Senior Data Scientist/Data Scientist',\n",
       " 'Data Scientist 2',\n",
       " 'Lead Data Scientist',\n",
       " 'Senior Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title = []\n",
    "\n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "for t in title_tags:\n",
    "    title = t.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "job_title = job_title[:10]\n",
    "job_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c3476",
   "metadata": {},
   "source": [
    "#### Extracting job location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b8cd479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Pune, Gurgaon/Gurugram, Chennai, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru(HSR Layout +2)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location = []\n",
    "\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "\n",
    "for loc in location_tags:\n",
    "    location = loc.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "job_location = job_location[:10]\n",
    "job_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602815c",
   "metadata": {},
   "source": [
    "#### Extracting company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5739ee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accenture',\n",
       " 'Walmart',\n",
       " 'Walmart',\n",
       " 'Swiggy',\n",
       " 'Tredence Analytics Solutions Private Limited',\n",
       " 'PayPal',\n",
       " 'Thoucentric Technology Pvt ltd',\n",
       " 'Flipkart',\n",
       " 'Medi Assist Insurance TPA Pvt. Ltd',\n",
       " 'Medi Assist Insurance TPA Pvt. Ltd']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_names = []\n",
    "\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "for com in company_tags:\n",
    "    company = com.text\n",
    "    company_names.append(company)\n",
    "company_names = company_names[:10]\n",
    "\n",
    "company_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d6935",
   "metadata": {},
   "source": [
    "Now we have scraped the required data from the website. We can check the length of elements in each list to create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6677e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc9a91",
   "metadata": {},
   "source": [
    "#### Closing the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3849adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a13e93",
   "metadata": {},
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad30c433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staff Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist II- Merchandise &amp; Discovery</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Swiggy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist/Data Scientist</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "      <td>Tredence Analytics Solutions Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru(HSR Layout +2)</td>\n",
       "      <td>Thoucentric Technology Pvt ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Medi Assist Insurance TPA Pvt. Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Medi Assist Insurance TPA Pvt. Ltd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title  \\\n",
       "0                       Junior Data Scientist   \n",
       "1                        Staff Data Scientist   \n",
       "2                              Data Scientist   \n",
       "3  Data Scientist II- Merchandise & Discovery   \n",
       "4        Senior Data Scientist/Data Scientist   \n",
       "5                            Data Scientist 2   \n",
       "6                         Lead Data Scientist   \n",
       "7                       Senior Data Scientist   \n",
       "8                              Data Scientist   \n",
       "9                              Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                 Bangalore/Bengaluru(HSR Layout +2)   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                        Company  \n",
       "0                                     Accenture  \n",
       "1                                       Walmart  \n",
       "2                                       Walmart  \n",
       "3                                        Swiggy  \n",
       "4  Tredence Analytics Solutions Private Limited  \n",
       "5                                        PayPal  \n",
       "6                Thoucentric Technology Pvt ltd  \n",
       "7                                      Flipkart  \n",
       "8            Medi Assist Insurance TPA Pvt. Ltd  \n",
       "9            Medi Assist Insurance TPA Pvt. Ltd  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = pd.DataFrame({})\n",
    "\n",
    "jobs['Title'] = job_title\n",
    "jobs['Location'] = job_location\n",
    "jobs['Company'] = company_names\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f8ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30f6a24e",
   "metadata": {},
   "source": [
    "### 3. Python program to scrape data for 'Data Scientist' desgination for location 'Delhi/NCR'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080b21",
   "metadata": {},
   "source": [
    "Tasks\n",
    "\n",
    "1. Website : http://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Click the search button\n",
    "4. Apply the location filter and salary filter by checking the respective boxes. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "5. Scrape the data for the first 10 jobs results you get.\n",
    "6. Create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920501a",
   "metadata": {},
   "source": [
    "Website = 'http://www.naukri.com/'\n",
    "\n",
    "Details to be scraped\n",
    "\n",
    "    1. job-title\n",
    "    2. job-location\n",
    "    3. company_name\n",
    "    4. experience required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15b2fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6be696ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver \n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a4c6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df305304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “Data Scientist” in “Skill, Designations, Companies” field.\n",
    "\n",
    "search_field_desgination = driver.find_element_by_class_name('suggestor-input ')\n",
    "search_field_desgination.send_keys('Data Scientist')\n",
    "\n",
    "#Clicking the search button\n",
    "\n",
    "search_button = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31a99c",
   "metadata": {},
   "source": [
    "#### Applying filter by location - Delhi/NCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fde40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_location_Delhi_NCR = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[3]/label/p/span[1]')\n",
    "filter_by_location_Delhi_NCR.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f2ba3",
   "metadata": {},
   "source": [
    "#### Applying filter by salary - 3 - 6 lakh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b80e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_salary = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]')\n",
    "filter_by_salary.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5ed0b",
   "metadata": {},
   "source": [
    "#### Extracting Job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f5fbd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hiring For Senior Data Scientist-Noida',\n",
       " 'Senior Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Data Scientist - Python/SQL',\n",
       " 'Data Scientist/ Machine Learning, 2022 Passout Can also apply',\n",
       " 'Data Scientist (freelance)',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Urgent Requirement || Data Scientist || Noida',\n",
       " 'AM Data Scientist - Goods & Service Tax Network, Delhi']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title = []\n",
    "\n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "for t in title_tags:\n",
    "    title = t.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "job_title = job_title[:10]\n",
    "job_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16ee10",
   "metadata": {},
   "source": [
    "#### Extracting job location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1c613bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noida, Greater Noida, Delhi / NCR',\n",
       " 'Noida',\n",
       " 'Gurgaon/Gurugram',\n",
       " 'Noida, Kota, Mumbai, Chandigarh, Lucknow, Agra, Gurgaon/Gurugram, Jaipur, Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'New Delhi, Delhi',\n",
       " 'Gurgaon, Bengaluru',\n",
       " 'New Delhi',\n",
       " 'Noida, Delhi / NCR',\n",
       " 'Delhi / NCR']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location = []\n",
    "\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "\n",
    "for loc in location_tags:\n",
    "    location = loc.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "job_location = job_location[:10]\n",
    "job_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ce1fc",
   "metadata": {},
   "source": [
    "#### Extracting company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b6736bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lumiq.ai',\n",
       " 'Optum',\n",
       " 'KIA INDIA PRIVATE LIMITED',\n",
       " 'AVE-Promagne',\n",
       " 'Creative Hands HR Consultancy',\n",
       " '2Coms',\n",
       " 'BlackBuck',\n",
       " 'Boston Consulting Group',\n",
       " 'HCL Technologies',\n",
       " 'NISG (National Institute for Smart Government)']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_names = []\n",
    "\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "for com in company_tags:\n",
    "    company = com.text\n",
    "    company_names.append(company)\n",
    "company_names = company_names[:10]\n",
    "\n",
    "company_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fca47",
   "metadata": {},
   "source": [
    "#### Extracting experience required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc54806a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-6 Yrs',\n",
       " '2-7 Yrs',\n",
       " '7-10 Yrs',\n",
       " '3-8 Yrs',\n",
       " '0-4 Yrs',\n",
       " '2-7 Yrs',\n",
       " '3-7 Yrs',\n",
       " '2-5 Yrs',\n",
       " '3-8 Yrs',\n",
       " '3-8 Yrs']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_required = []\n",
    "\n",
    "experience_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "\n",
    "for exp in experience_tags:\n",
    "    experience = exp.text\n",
    "    experience_required.append(experience)\n",
    "    \n",
    "experience_required = experience_required[:10]\n",
    "experience_required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5604be",
   "metadata": {},
   "source": [
    "Now we have scraped the required data from the website. We can check the length of elements in each list to create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bf599e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_names),len(experience_required))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9c594",
   "metadata": {},
   "source": [
    "#### Closing the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe7be619",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13c65f",
   "metadata": {},
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a229ece3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiring For Senior Data Scientist-Noida</td>\n",
       "      <td>Noida, Greater Noida, Delhi / NCR</td>\n",
       "      <td>Lumiq.ai</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>KIA INDIA PRIVATE LIMITED</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Python/SQL</td>\n",
       "      <td>Noida, Kota, Mumbai, Chandigarh, Lucknow, Agra...</td>\n",
       "      <td>AVE-Promagne</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist/ Machine Learning, 2022 Passout...</td>\n",
       "      <td>Hyderabad/Secunderabad, Ahmedabad, Chennai, Ba...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist (freelance)</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "      <td>2Coms</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon, Bengaluru</td>\n",
       "      <td>BlackBuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Urgent Requirement || Data Scientist || Noida</td>\n",
       "      <td>Noida, Delhi / NCR</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AM Data Scientist - Goods &amp; Service Tax Networ...</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>NISG (National Institute for Smart Government)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0             Hiring For Senior Data Scientist-Noida   \n",
       "1                              Senior Data Scientist   \n",
       "2                                Lead Data Scientist   \n",
       "3                        Data Scientist - Python/SQL   \n",
       "4  Data Scientist/ Machine Learning, 2022 Passout...   \n",
       "5                         Data Scientist (freelance)   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8      Urgent Requirement || Data Scientist || Noida   \n",
       "9  AM Data Scientist - Goods & Service Tax Networ...   \n",
       "\n",
       "                                            Location  \\\n",
       "0                  Noida, Greater Noida, Delhi / NCR   \n",
       "1                                              Noida   \n",
       "2                                   Gurgaon/Gurugram   \n",
       "3  Noida, Kota, Mumbai, Chandigarh, Lucknow, Agra...   \n",
       "4  Hyderabad/Secunderabad, Ahmedabad, Chennai, Ba...   \n",
       "5                                   New Delhi, Delhi   \n",
       "6                                 Gurgaon, Bengaluru   \n",
       "7                                          New Delhi   \n",
       "8                                 Noida, Delhi / NCR   \n",
       "9                                        Delhi / NCR   \n",
       "\n",
       "                                          Company Experience Required  \n",
       "0                                        Lumiq.ai             2-6 Yrs  \n",
       "1                                           Optum             2-7 Yrs  \n",
       "2                       KIA INDIA PRIVATE LIMITED            7-10 Yrs  \n",
       "3                                    AVE-Promagne             3-8 Yrs  \n",
       "4                   Creative Hands HR Consultancy             0-4 Yrs  \n",
       "5                                           2Coms             2-7 Yrs  \n",
       "6                                       BlackBuck             3-7 Yrs  \n",
       "7                         Boston Consulting Group             2-5 Yrs  \n",
       "8                                HCL Technologies             3-8 Yrs  \n",
       "9  NISG (National Institute for Smart Government)             3-8 Yrs  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = pd.DataFrame({})\n",
    "\n",
    "jobs['Title'] = job_title\n",
    "jobs['Location'] = job_location\n",
    "jobs['Company'] = company_names\n",
    "jobs['Experience Required'] = experience_required\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82910edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eb64a2f",
   "metadata": {},
   "source": [
    "### 4. Python Program to Scrape data of first 100 sunglasses listings on flipkart.com."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3933da0",
   "metadata": {},
   "source": [
    "Attributes to be scraped:\n",
    "    \n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed91296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbe7fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4120fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7ce2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the popup\n",
    "close_popup_button = driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "close_popup_button.click()\n",
    "#Entering \"Sunglasses\" in the search field\n",
    "\n",
    "search_field = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search_field.send_keys('Sunglasses')\n",
    "\n",
    "#Clicking the search button\n",
    "\n",
    "search_button = driver.find_element_by_xpath('/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f5c8a",
   "metadata": {},
   "source": [
    "#### Extracting attributes from first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec695fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#Extracting the brand names\n",
    "brand_names = []\n",
    "\n",
    "brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "print(len(brand_names))\n",
    "\n",
    "#Extracting product description\n",
    "product_description = []\n",
    "\n",
    "product_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "\n",
    "for pr in product_tags:\n",
    "    product = pr.text\n",
    "    product_description.append(product)   \n",
    "print(len(product_description))\n",
    "\n",
    "#Extracting Price\n",
    "product_price = []\n",
    "\n",
    "price_tags = driver.find_elements_by_class_name('_30jeq3')\n",
    "discount_tags = driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "\n",
    "for p, d in zip(price_tags,discount_tags):\n",
    "    price = p.text\n",
    "    discount = d.text\n",
    "    product_price.append([price,discount])\n",
    "product_price = product_price[:40]\n",
    "\n",
    "print(len(product_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7e6aa",
   "metadata": {},
   "source": [
    "Now we have scraped 40 attributes of brands,product description,price from the first page. Now lets click the 'NEXT' button on the page to scrape attributes from second page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bb6f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Page button\n",
    "\n",
    "next_page_button = driver.find_element_by_xpath('/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span')\n",
    "next_page_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9bd25",
   "metadata": {},
   "source": [
    "#### Extracting attributes from second page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c44a5752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#Extracting brand names\n",
    "brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "print(len(brand_names))\n",
    "\n",
    "#Extracting product description\n",
    "product_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "\n",
    "for pr in product_tags:\n",
    "    product = pr.text\n",
    "    product_description.append(product)\n",
    "    \n",
    "print(len(product_description))\n",
    "\n",
    "#Extracting Price\n",
    "price_tags = driver.find_elements_by_class_name('_30jeq3')\n",
    "discount_tags = driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "\n",
    "for p, d in zip(price_tags,discount_tags):\n",
    "    price = p.text\n",
    "    discount = d.text\n",
    "    product_price.append([price,discount])\n",
    "product_price = product_price[:80]\n",
    "\n",
    "print(len(product_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe413fba",
   "metadata": {},
   "source": [
    "Now we have scraped 80 attributes of brands,product description,price from the first page. Now lets click the 'NEXT' button on the page to scrape attributes from third page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c27a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Page button\n",
    "\n",
    "next_page_button = driver.find_element_by_xpath('/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span')\n",
    "next_page_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fe784",
   "metadata": {},
   "source": [
    "#### Extracting attributes form third page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a283401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting brand names\n",
    "brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "    \n",
    "brand_names = brand_names[:100]\n",
    "\n",
    "#Extracting product description\n",
    "product_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "\n",
    "for pr in product_tags:\n",
    "    product = pr.text\n",
    "    product_description.append(product)\n",
    "    \n",
    "product_description = product_description[:100]\n",
    "\n",
    "#Extracting Price\n",
    "price_tags = driver.find_elements_by_class_name('_30jeq3')\n",
    "discount_tags = driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "\n",
    "for p, d in zip(price_tags,discount_tags):\n",
    "    price = p.text\n",
    "    discount = d.text\n",
    "    product_price.append([price,discount])\n",
    "    \n",
    "product_price = product_price[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7833ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(brand_names),len(product_description),len(product_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae508a7",
   "metadata": {},
   "source": [
    "#### Closing the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eaf021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce590a4",
   "metadata": {},
   "source": [
    "Now we have scraped 100 attributes for the required fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5411f1",
   "metadata": {},
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebf9f066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price and Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john jacobs</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (47)</td>\n",
       "      <td>[₹3,850, 35% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (59)</td>\n",
       "      <td>[₹599, 70% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>[₹199, 84% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>[₹709, 21% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>[₹208, 91% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>Toughened Glass Lens, UV Protection Rectangula...</td>\n",
       "      <td>[₹725, 77% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>[₹424, 80% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Mi</td>\n",
       "      <td>Polarized Aviator Sunglasses (Free Size)</td>\n",
       "      <td>[₹889, 25% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Rectangular, Retro Square Sungla...</td>\n",
       "      <td>[₹424, 71% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Round Sun...</td>\n",
       "      <td>[₹949, 52% off]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description  \\\n",
       "0      john jacobs           UV Protection Clubmaster Sunglasses (47)   \n",
       "1    VINCENT CHASE             UV Protection Wayfarer Sunglasses (59)   \n",
       "2             SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "3         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "4        Elligator                UV Protection Round Sunglasses (54)   \n",
       "..             ...                                                ...   \n",
       "95          AISLIN  Toughened Glass Lens, UV Protection Rectangula...   \n",
       "96  ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)   \n",
       "97              Mi           Polarized Aviator Sunglasses (Free Size)   \n",
       "98       ROYAL SON  UV Protection Rectangular, Retro Square Sungla...   \n",
       "99   VINCENT CHASE  by Lenskart Polarized, UV Protection Round Sun...   \n",
       "\n",
       "   Price and Discount  \n",
       "0   [₹3,850, 35% off]  \n",
       "1     [₹599, 70% off]  \n",
       "2     [₹199, 84% off]  \n",
       "3     [₹709, 21% off]  \n",
       "4     [₹208, 91% off]  \n",
       "..                ...  \n",
       "95    [₹725, 77% off]  \n",
       "96    [₹424, 80% off]  \n",
       "97    [₹889, 25% off]  \n",
       "98    [₹424, 71% off]  \n",
       "99    [₹949, 52% off]  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_details = pd.DataFrame({})\n",
    "\n",
    "product_details['Brand'] = brand_names\n",
    "product_details['Product Description'] = product_description\n",
    "product_details['Price and Discount'] = product_price\n",
    "\n",
    "product_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83c0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f28474d2",
   "metadata": {},
   "source": [
    "### 5. Python program to Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a1378",
   "metadata": {},
   "source": [
    "Details to be scraped\n",
    "\n",
    "1. Rating\n",
    "2. Review Summary\n",
    "3. Full Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7c77852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86aa8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the web driver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f07b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f599be",
   "metadata": {},
   "source": [
    "From the product description page, we have to get the product review page. For that we have to click on the \"All reviews\" page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd7fce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review_button = driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/a/div/span')\n",
    "all_review_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92bdde",
   "metadata": {},
   "source": [
    "#### Extracting attributes from first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd866369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Extracting ratings\n",
    "rating_list = []\n",
    "\n",
    "rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "\n",
    "for rate in rating_tag:\n",
    "    rating = rate.text\n",
    "    rating_list.append(rating)\n",
    "    \n",
    "print(len(rating_list))\n",
    "\n",
    "#Extracting review summary\n",
    "review_summary = []\n",
    "\n",
    "review_tags = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "\n",
    "for rev in review_tags:\n",
    "    review = rev.text\n",
    "    review_summary.append(review)\n",
    "    \n",
    "print(len(review_summary))\n",
    "\n",
    "#Extracting full review\n",
    "full_review = []\n",
    "\n",
    "full_review_tags = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "\n",
    "for re in full_review_tags:\n",
    "    full_rev = re.text\n",
    "    full_review.append(full_rev)\n",
    "    \n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13e1bf",
   "metadata": {},
   "source": [
    "Now we have scraped 10 elements for each attributs from the first page. Now lets scrape the rest of 90 elements from the following pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0e66eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the next button for loading the second page\n",
    "next_button = driver.find_element_by_xpath(\"//a[@class='_1LKTO3'][1]\")\n",
    "next_button.click()\n",
    "next_button = driver.find_element_by_xpath(\"//a[@class='_1LKTO3'][1]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "620a20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "#Creating a forloop to scrape all the details from following pages to make 100 elements for each attributes\n",
    "for page in range(2,11,1):\n",
    "    urla = driver.current_url[:-1]\n",
    "    driver.get(urla+str(page))\n",
    "    \n",
    "    rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq' or @class='_3LWZlK _1rdVr6 _1BLPMq']\")\n",
    "    for rate in rating_tag:\n",
    "        rating = rate.text\n",
    "        rating_list.append(rating)\n",
    "        \n",
    "    review_tags = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for rev in review_tags:\n",
    "        review = rev.text\n",
    "        review_summary.append(review)\n",
    "        \n",
    "    full_review_tags = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for re in full_review_tags:\n",
    "        full_rev = re.text\n",
    "        full_review.append(full_rev)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(len(rating_list),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794665aa",
   "metadata": {},
   "source": [
    "Now we have scraped 100 elements for each attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac4b5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beda49",
   "metadata": {},
   "source": [
    "Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8443b6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Pretty good</td>\n",
       "      <td>I've used this phone for over a month now and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Excellent camera 📸 And Display touching very N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>I purchased the iPhone 11 a month back. I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>The ultimate performance\\nCamera is superb\\nTh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating         Review Summary  \\\n",
       "0       5              Brilliant   \n",
       "1       5         Simply awesome   \n",
       "2       5    Best in the market!   \n",
       "3       5       Perfect product!   \n",
       "4       5              Fabulous!   \n",
       "..    ...                    ...   \n",
       "95      4            Pretty good   \n",
       "96      5      Worth every penny   \n",
       "97      5  Mind-blowing purchase   \n",
       "98      5              Fabulous!   \n",
       "99      5              Just wow!   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  I've used this phone for over a month now and ...  \n",
       "96  Undoubtedly Iphone 11 is the most successful m...  \n",
       "97  Excellent camera 📸 And Display touching very N...  \n",
       "98  I purchased the iPhone 11 a month back. I must...  \n",
       "99  The ultimate performance\\nCamera is superb\\nTh...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iphone_review = pd.DataFrame({})\n",
    "iphone_review['Rating']= rating_list\n",
    "iphone_review['Review Summary'] = review_summary\n",
    "iphone_review['Full Review'] = full_review\n",
    "\n",
    "iphone_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106f2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b57661ca",
   "metadata": {},
   "source": [
    "### 6. Python program to scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40c9bb",
   "metadata": {},
   "source": [
    "Details to be scraped \n",
    "\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "998d7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "858c1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d10f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "937a826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the popup\n",
    "close_popup_button = driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "close_popup_button.click()\n",
    "#Entering \"sneakers\" in the search field\n",
    "\n",
    "search_field = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search_field.send_keys('sneakers')\n",
    "\n",
    "#Clicking the search button\n",
    "\n",
    "search_button = driver.find_element_by_xpath('/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa997e",
   "metadata": {},
   "source": [
    "#### Extracting attributes from first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2373f5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#Extracting the brand names\n",
    "brand_names = []\n",
    "\n",
    "brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "print(len(brand_names))\n",
    "\n",
    "#Extracting product description\n",
    "product_description = []\n",
    "\n",
    "product_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa' or @class='IRpwTa _2-ICcC']\")\n",
    "\n",
    "for pr in product_tags:\n",
    "    product = pr.text\n",
    "    product_description.append(product)   \n",
    "print(len(product_description))\n",
    "\n",
    "#Extracting Price\n",
    "product_price = []\n",
    "\n",
    "price_tags = driver.find_elements_by_class_name('_30jeq3')\n",
    "discount_tags = driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "\n",
    "for p, d in zip(price_tags,discount_tags):\n",
    "    price = p.text\n",
    "    discount = d.text\n",
    "    product_price.append([price,discount])\n",
    "product_price = product_price[:40]\n",
    "\n",
    "print(len(product_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52852d2c",
   "metadata": {},
   "source": [
    "Now we have scraped 40 attributes of brands,product description,price from the first page. Now lets click the 'NEXT' button on the page to scrape attributes from second page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74bd646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Page button\n",
    "\n",
    "next_page_button = driver.find_element_by_xpath('/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span')\n",
    "next_page_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d921ffa",
   "metadata": {},
   "source": [
    "#### Extracting attributes from second page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4768a880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#Extracting brand names\n",
    "brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "print(len(brand_names))\n",
    "\n",
    "#Extracting product description\n",
    "product_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa' or @class='IRpwTa _2-ICcC']\")\n",
    "\n",
    "for pr in product_tags:\n",
    "    product = pr.text\n",
    "    product_description.append(product)\n",
    "    \n",
    "print(len(product_description))\n",
    "\n",
    "#Extracting Price\n",
    "price_tags = driver.find_elements_by_class_name('_30jeq3')\n",
    "discount_tags = driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "\n",
    "for p, d in zip(price_tags,discount_tags):\n",
    "    price = p.text\n",
    "    discount = d.text\n",
    "    product_price.append([price,discount])\n",
    "product_price = product_price[:80]\n",
    "\n",
    "print(len(product_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecf2a1",
   "metadata": {},
   "source": [
    "Now we have scraped 80 attributes of brands,product description,price from the first page. Now lets click the 'NEXT' button on the page to scrape attributes from third page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd65e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Page button\n",
    "\n",
    "next_page_button = driver.find_element_by_xpath('/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span')\n",
    "next_page_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c982e",
   "metadata": {},
   "source": [
    "#### Extracting attributes form third page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d18f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting brand names\n",
    "brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "    \n",
    "brand_names = brand_names[:100]\n",
    "\n",
    "#Extracting product description\n",
    "product_tags = driver.find_elements_by_xpath(\"//a[@class='IRpwTa' or @class='IRpwTa _2-ICcC']\")\n",
    "\n",
    "for pr in product_tags:\n",
    "    product = pr.text\n",
    "    product_description.append(product)\n",
    "    \n",
    "product_description = product_description[:100]\n",
    "\n",
    "#Extracting Price\n",
    "price_tags = driver.find_elements_by_class_name('_30jeq3')\n",
    "discount_tags = driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "\n",
    "for p, d in zip(price_tags,discount_tags):\n",
    "    price = p.text\n",
    "    discount = d.text\n",
    "    product_price.append([price,discount])\n",
    "    \n",
    "product_price = product_price[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd7f6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(brand_names),len(product_description),len(product_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a388da",
   "metadata": {},
   "source": [
    "#### Closing the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "397eaa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ccfe1",
   "metadata": {},
   "source": [
    "Now we have scraped 100 attributes for the required fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edcea8",
   "metadata": {},
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa4c6cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price and Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T-ROCK</td>\n",
       "      <td>Stylish White Casual Sneakers shoes Men and Bo...</td>\n",
       "      <td>[₹399, 60% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HIGHLANDER</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>[₹945, 52% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>[₹469, 70% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>[₹349, 82% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>[₹374, 62% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>HIGHLANDER</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>[₹945, 52% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>luxury fashion</td>\n",
       "      <td>Fashionable sneaker shoes Sneakers For Men</td>\n",
       "      <td>[₹449, 77% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Xtoon</td>\n",
       "      <td>white Casual shoes,Sneakers for men's Sneakers...</td>\n",
       "      <td>[₹448, 55% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>[₹309, 84% off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>[₹379, 70% off]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description  \\\n",
       "0           T-ROCK  Stylish White Casual Sneakers shoes Men and Bo...   \n",
       "1       HIGHLANDER                                   Sneakers For Men   \n",
       "2           Chevit  Super Stylish & Trendy Combo Pack of 02 Pairs ...   \n",
       "3         KWIK FIT  Kwik FIT casual sneaker shoes and partywear sh...   \n",
       "4         Magnolia                                   Sneakers For Men   \n",
       "..             ...                                                ...   \n",
       "95      HIGHLANDER                                   Sneakers For Men   \n",
       "96  luxury fashion         Fashionable sneaker shoes Sneakers For Men   \n",
       "97           Xtoon  white Casual shoes,Sneakers for men's Sneakers...   \n",
       "98            aadi                                   Sneakers For Men   \n",
       "99         Numenzo                                   Sneakers For Men   \n",
       "\n",
       "   Price and Discount  \n",
       "0     [₹399, 60% off]  \n",
       "1     [₹945, 52% off]  \n",
       "2     [₹469, 70% off]  \n",
       "3     [₹349, 82% off]  \n",
       "4     [₹374, 62% off]  \n",
       "..                ...  \n",
       "95    [₹945, 52% off]  \n",
       "96    [₹449, 77% off]  \n",
       "97    [₹448, 55% off]  \n",
       "98    [₹309, 84% off]  \n",
       "99    [₹379, 70% off]  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_details = pd.DataFrame({})\n",
    "\n",
    "product_details['Brand'] = brand_names\n",
    "product_details['Product Description'] = product_description\n",
    "product_details['Price and Discount'] = product_price\n",
    "\n",
    "product_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5871757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21e9574",
   "metadata": {},
   "source": [
    "### 7. Python program to scrape First 100 shoes data from Myntra.com/shoes. Set Price filter to “Rs. 7149 to Rs. 14099 ”, Color filter to “Black”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d01c0",
   "metadata": {},
   "source": [
    "Details to Scrape\n",
    "\n",
    "1. Brand\n",
    "2. Short Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7bf72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64d4f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5831d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the website\n",
    "url = 'http://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e9e40",
   "metadata": {},
   "source": [
    "#### Setting the filter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4b1f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the price range\n",
    "\n",
    "filter_price_button = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label')\n",
    "filter_price_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e04fd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the required color\n",
    "\n",
    "filter_color_button = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label')\n",
    "filter_color_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25e5f3",
   "metadata": {},
   "source": [
    "#### Extracting the elements for each attributs from first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4995f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#Extracting brand names\n",
    "brand_names =[]\n",
    "brand_tags = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "print(len(brand_names))\n",
    "      \n",
    "#Extracting short description\n",
    "short_des = []\n",
    "short_des_tags = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "\n",
    "for sh in short_des_tags:\n",
    "    short = sh.text\n",
    "    short_des.append(short)\n",
    "print(len(short_des))\n",
    "\n",
    "#Extracting shoe price\n",
    "shoe_price = []\n",
    "price_tags = driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "\n",
    "for pr in price_tags:\n",
    "    price = pr.text\n",
    "    shoe_price.append(price)\n",
    "print(len(shoe_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7f5db",
   "metadata": {},
   "source": [
    "Now we can initiate the next button to scrape the data from next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2a6b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page_button = driver.find_element_by_xpath(\"//li[@class='pagination-next']\")\n",
    "next_page_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0337b1",
   "metadata": {},
   "source": [
    "#### Extracting the elements for each attributes from second page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d3a3949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracting brand names\n",
    "brand_tags = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "\n",
    "for br in brand_tags:\n",
    "    brand = br.text\n",
    "    brand_names.append(brand)\n",
    "print(len(brand_names))\n",
    "      \n",
    "#Extracting short description\n",
    "short_des_tags = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "\n",
    "for sh in short_des_tags:\n",
    "    short = sh.text\n",
    "    short_des.append(short)\n",
    "print(len(short_des))\n",
    "\n",
    "#Extracting shoe price\n",
    "price_tags = driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "\n",
    "for pr in price_tags:\n",
    "    price = pr.text\n",
    "    shoe_price.append(price)\n",
    "print(len(shoe_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570dd8c",
   "metadata": {},
   "source": [
    "We have scraped data for 100 shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9d7b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254753f1",
   "metadata": {},
   "source": [
    "#### Creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ecb276a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Leather Loafers</td>\n",
       "      <td>Rs. 11199Rs. 15999(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Leather Driving Shoes</td>\n",
       "      <td>Rs. 9099Rs. 12999(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men JORDAN ONE Basketball</td>\n",
       "      <td>Rs. 8295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Printed Sneakers</td>\n",
       "      <td>Rs. 9099Rs. 12999(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Fly 4 Running Shoes</td>\n",
       "      <td>Rs. 13045Rs. 14495(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Solid Leather Ballerinas</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Slip-On Formal Moccasins</td>\n",
       "      <td>Rs. 9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Women Leather Open Toe Flats</td>\n",
       "      <td>Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Formal Derbys</td>\n",
       "      <td>Rs. 9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Suede Loafers</td>\n",
       "      <td>Rs. 12990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Brand             Product Description  \\\n",
       "0                   ALDO             Men Leather Loafers   \n",
       "1                   ALDO       Men Leather Driving Shoes   \n",
       "2                   Nike       Men JORDAN ONE Basketball   \n",
       "3                   ALDO            Men Printed Sneakers   \n",
       "4                   Nike    Men Zoom Fly 4 Running Shoes   \n",
       "..                   ...                             ...   \n",
       "95                  Geox  Women Solid Leather Ballerinas   \n",
       "96                  Geox    Men Slip-On Formal Moccasins   \n",
       "97             Cole Haan    Women Leather Open Toe Flats   \n",
       "98                  Geox               Men Formal Derbys   \n",
       "99  Heel & Buckle London               Men Suede Loafers   \n",
       "\n",
       "                          Price  \n",
       "0   Rs. 11199Rs. 15999(30% OFF)  \n",
       "1    Rs. 9099Rs. 12999(30% OFF)  \n",
       "2                      Rs. 8295  \n",
       "3    Rs. 9099Rs. 12999(30% OFF)  \n",
       "4   Rs. 13045Rs. 14495(10% OFF)  \n",
       "..                          ...  \n",
       "95                     Rs. 8999  \n",
       "96                     Rs. 9990  \n",
       "97                    Rs. 12999  \n",
       "98                     Rs. 9990  \n",
       "99                    Rs. 12990  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "shoes  = pd.DataFrame({})\n",
    "shoes['Brand'] = brand_names\n",
    "shoes['Product Description'] = short_des\n",
    "shoes['Price'] = shoe_price\n",
    "\n",
    "shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854602a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17ea2d36",
   "metadata": {},
   "source": [
    "### 8. Python program to scrape data from http://www.amazon.in/ after setting the filters to scrape first 10 laptops data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9eb700",
   "metadata": {},
   "source": [
    "Input data\n",
    "\n",
    "1. Enter “Laptop” in the search field and then click the search icon.\n",
    "2. Set CPU Type filter to “Intel Core i7” and “Intel Core i9”\n",
    "\n",
    "Details to be scraped\n",
    "\n",
    "1. Title\n",
    "2. Rating\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fdefe78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "11157453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68918b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the website\n",
    "url = 'http://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d281ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search input\n",
    "\n",
    "search_field_input = driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')\n",
    "search_field_input.send_keys('Laptop')\n",
    "\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf91b9f",
   "metadata": {},
   "source": [
    "#### Setting the filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57fdad",
   "metadata": {},
   "source": [
    "#### Data for filter - Intel Core i7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "525e8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cpu_i7 = driver.find_element_by_xpath('//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/div/label/i')\n",
    "filter_cpu_i7.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f0440",
   "metadata": {},
   "source": [
    "#### Extracting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e64124d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Extracting titles\n",
    "\n",
    "titles = []\n",
    "title_tags = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "\n",
    "for t in title_tags:\n",
    "    title = t.text\n",
    "    titles.append(title)\n",
    "titles = titles[:10]\n",
    "print(len(titles))\n",
    "#Extracting Ratings\n",
    "ratings = []\n",
    "rating_tags = driver.find_elements_by_xpath(\"//div[@class='a-row a-size-small']/span\")\n",
    "\n",
    "for ra in rating_tags:\n",
    "    rating = ra.get_attribute('aria-label')\n",
    "    ratings.append(rating)\n",
    "ratings = ratings[:10]\n",
    "print(len(ratings))    \n",
    "\n",
    "#Extracting Price\n",
    "\n",
    "prices = []\n",
    "price_tags = driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for pr in price_tags:\n",
    "    price = pr.text\n",
    "    prices.append(price)\n",
    "prices = prices[:10]\n",
    "print(len(prices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc89b0c",
   "metadata": {},
   "source": [
    "We have scraped the first 10 data for filter - Intel Core i7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4c85522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launching the page with search input laptops without filter\n",
    "\n",
    "#Loading the website\n",
    "url = 'http://www.amazon.in/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416e8e2",
   "metadata": {},
   "source": [
    "#### Since the intel core i9 filter is not available. we are changing the categories to \"Computers & Accessories\" for making the filter available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f7c5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search input\n",
    "\n",
    "search_field_input = driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')\n",
    "search_field_input.send_keys('Laptop')\n",
    "\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed9a17",
   "metadata": {},
   "source": [
    "#### Data for filter - Intel Core i9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6c11073",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cpu_i9 = driver.find_element_by_xpath('//*[@id=\"p_n_feature_thirteen_browse-bin/16757432031\"]/span/a/div/label/i')\n",
    "filter_cpu_i9.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c55b43",
   "metadata": {},
   "source": [
    "#### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc78db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the data is not showing up since the items are not in stock, we can include out of stock items.\n",
    "include_outofstock_button = driver.find_element_by_xpath(\"//*[@id='p_n_availability/1318485031']/span/a/div/label/i\")\n",
    "include_outofstock_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba96bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Extracting titles\n",
    "\n",
    "\n",
    "title_tags = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "\n",
    "for t in title_tags:\n",
    "    title = t.text\n",
    "    titles.append(title)\n",
    "titles = titles[:20]\n",
    "print(len(titles))\n",
    "#Extracting Ratings\n",
    "\n",
    "rating_tags = driver.find_elements_by_xpath(\"//div[@class='a-row a-size-small']/span\")\n",
    "\n",
    "for ra in rating_tags:\n",
    "    rating = ra.get_attribute('aria-label')\n",
    "    ratings.append(rating)\n",
    "ratings = ratings[:20]\n",
    "print(len(ratings))    \n",
    "\n",
    "#Extracting Price\n",
    "\n",
    "\n",
    "price_tags = driver.find_elements_by_xpath(\"//div[@class='sg-col-inner']/div/div/a/span\")\n",
    "for pr in price_tags:\n",
    "    price = pr.text\n",
    "    prices.append(price)\n",
    "prices = prices[:20]\n",
    "print(len(prices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb82c9",
   "metadata": {},
   "source": [
    "Now we have collected information for laptops with  i7 and i9 as filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3bd8cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd41f12",
   "metadata": {},
   "source": [
    "#### Creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "60006c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>1,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy Book2 360 Intel 12th Gen i7 Evo...</td>\n",
       "      <td>3</td>\n",
       "      <td>1,05,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...</td>\n",
       "      <td>55</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LG Gram 16 inches Intel Evo 11th Gen Core i7 U...</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>89,611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>38</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>94,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Samsung Galaxy Book2 360 Intel 12th Gen i7 Evo...</td>\n",
       "      <td>82</td>\n",
       "      <td>1,05,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ASUS ROG Zephyrus M16 (2022), 16-inch (40.64 c...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>₹3,31,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ASUS ROG Strix SCAR 15 (2022), 15.6-inch (39.6...</td>\n",
       "      <td>21</td>\n",
       "      <td>₹3,89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ASUS ROG Strix Scar 15 (2022), 15.6-inch (39.6...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>₹3,15,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HP Omen X 2S Intel Core i9 9th Gen 15.6 inches...</td>\n",
       "      <td>11</td>\n",
       "      <td>₹3,76,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>₹2,90,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Acer Predator Helios 300 11th Gen Intel Core i...</td>\n",
       "      <td>1</td>\n",
       "      <td>₹3,17,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Renewed) HP Omen 15-dh0139TX Gaming Laptop (9...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹1,77,885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MSI Gaming Raider GE76, Intel 12th Gen. i9-129...</td>\n",
       "      <td>1</td>\n",
       "      <td>₹1,88,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ASUS ROG Strix Scar 17 (2022), 17.3-inch (43.9...</td>\n",
       "      <td>2.8 out of 5 stars</td>\n",
       "      <td>₹1,44,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ASUS ZenBook Pro Duo 15 OLED (2021) Intel Core...</td>\n",
       "      <td>2</td>\n",
       "      <td>₹1,69,999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title             Ratings  \\\n",
       "1   Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...  5.0 out of 5 stars   \n",
       "2   Samsung Galaxy Book2 360 Intel 12th Gen i7 Evo...                   3   \n",
       "3   ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...  4.5 out of 5 stars   \n",
       "4   HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...                  55   \n",
       "5   Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  5.0 out of 5 stars   \n",
       "6   Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...                   1   \n",
       "7   LG Gram 16 inches Intel Evo 11th Gen Core i7 U...  4.6 out of 5 stars   \n",
       "8   ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...                  38   \n",
       "9   LG Gram Intel Evo 11th Gen Core i7 17 inches U...  4.1 out of 5 stars   \n",
       "10  Samsung Galaxy Book2 360 Intel 12th Gen i7 Evo...                  82   \n",
       "11  ASUS ROG Zephyrus M16 (2022), 16-inch (40.64 c...  4.3 out of 5 stars   \n",
       "12  ASUS ROG Strix SCAR 15 (2022), 15.6-inch (39.6...                  21   \n",
       "13  ASUS ROG Strix Scar 15 (2022), 15.6-inch (39.6...  4.3 out of 5 stars   \n",
       "14  HP Omen X 2S Intel Core i9 9th Gen 15.6 inches...                  11   \n",
       "15  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...  1.0 out of 5 stars   \n",
       "16  Acer Predator Helios 300 11th Gen Intel Core i...                   1   \n",
       "17  (Renewed) HP Omen 15-dh0139TX Gaming Laptop (9...  5.0 out of 5 stars   \n",
       "18  MSI Gaming Raider GE76, Intel 12th Gen. i9-129...                   1   \n",
       "19  ASUS ROG Strix Scar 17 (2022), 17.3-inch (43.9...  2.8 out of 5 stars   \n",
       "20  ASUS ZenBook Pro Duo 15 OLED (2021) Intel Core...                   2   \n",
       "\n",
       "        Price  \n",
       "1    1,29,990  \n",
       "2    1,05,990  \n",
       "3      57,490  \n",
       "4      86,990  \n",
       "5      86,990  \n",
       "6      81,990  \n",
       "7      89,611  \n",
       "8      89,990  \n",
       "9      94,999  \n",
       "10   1,05,990  \n",
       "11  ₹3,31,990  \n",
       "12  ₹3,89,990  \n",
       "13  ₹3,15,100  \n",
       "14  ₹3,76,990  \n",
       "15  ₹2,90,990  \n",
       "16  ₹3,17,990  \n",
       "17  ₹1,77,885  \n",
       "18  ₹1,88,990  \n",
       "19  ₹1,44,990  \n",
       "20  ₹1,69,999  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop= pd.DataFrame({}, index= range(1,len(titles)+1))\n",
    "laptop['Title'] = titles\n",
    "laptop['Ratings'] = ratings\n",
    "laptop['Price'] = prices\n",
    "\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a14a9",
   "metadata": {},
   "source": [
    "First 10 results are for laptops with intel core i7 and next 10 results are for laptops with intel core i9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5d95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d66f28e8",
   "metadata": {},
   "source": [
    "### 9. Python program to scrape data for first 10 job results for Data Scientist Designation in Noida location from http://www.ambitionbox.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6afc431",
   "metadata": {},
   "source": [
    "Designation = Data Scientist\n",
    "Location - Noida\n",
    "\n",
    "Details to be scraped\n",
    "\n",
    "1. Company name\n",
    "2. No. of days ago job was posted\n",
    "3. Rating of the company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67fddfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6881aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "93ffa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the website\n",
    "url = 'http://www.ambitionbox.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a6189459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the job button\n",
    "job_page_button = driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[6]\")\n",
    "job_page_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ead5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search input\n",
    "\n",
    "search_field_designation = driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input\")\n",
    "search_field_designation.send_keys('Data Scientist')\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5d1db722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the location\n",
    "location_button = driver.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[1]/p')\n",
    "location_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a98d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching the location\n",
    "search_field_location = driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input\")\n",
    "search_field_location.send_keys(\"Noida\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5842ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_location_button = driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label\")\n",
    "search_location_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da85a7",
   "metadata": {},
   "source": [
    "#### Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c2f0b495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Extracting company names\n",
    "company_names = []\n",
    "company_names_tags = driver.find_elements_by_xpath(\"//div[@class='company-info']/p\")\n",
    "for com in company_names_tags:\n",
    "    company = com.text\n",
    "    company_names.append(company)\n",
    "company_names = company_names[1:]\n",
    "print(len(company_names))\n",
    "\n",
    "#Extracting no.of days ago when job was posted\n",
    "\n",
    "days_ago_posted = []\n",
    "days_ago_tags = driver.find_elements_by_xpath(\"//span[@class='body-small-l'][1]\")\n",
    "for day in days_ago_tags:\n",
    "    days = day.text\n",
    "    days_ago_posted.append(days)\n",
    "print(len(days_ago_posted))\n",
    "\n",
    "#Extracting Rating of the company\n",
    "company_rating = []\n",
    "company_rating_tags = driver.find_elements_by_xpath(\"//span[@class='body-small']\")\n",
    "for comp in company_rating_tags:\n",
    "    companyrating = comp.text\n",
    "    company_rating.append(companyrating)\n",
    "print(len(company_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b57f9a",
   "metadata": {},
   "source": [
    "We have scraped all the required data from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cc273714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56274bc3",
   "metadata": {},
   "source": [
    "#### Creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "73222198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No. of days ago when job was posted</th>\n",
       "      <th>Rating of the company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>21hr ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tech Mahindra Ltd</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>17d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>21d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Newgen Software Technologies Ltd.</td>\n",
       "      <td>1d ago</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JK Technosoft Ltd</td>\n",
       "      <td>10d ago</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pitney Bowes India Pvt Ltd</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microsoft India (R and D) Pvt Ltd</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Company Name  \\\n",
       "0                   GENPACT India Private Limited   \n",
       "1  Optum Global Solutions (India) Private Limited   \n",
       "2                               Tech Mahindra Ltd   \n",
       "3                   GENPACT India Private Limited   \n",
       "4                                HCL Technologies   \n",
       "5  Optum Global Solutions (India) Private Limited   \n",
       "6               Newgen Software Technologies Ltd.   \n",
       "7                               JK Technosoft Ltd   \n",
       "8                      Pitney Bowes India Pvt Ltd   \n",
       "9               Microsoft India (R and D) Pvt Ltd   \n",
       "\n",
       "  No. of days ago when job was posted Rating of the company  \n",
       "0                            21hr ago                   4.0  \n",
       "1                             14d ago                   4.1  \n",
       "2                             14d ago                   3.7  \n",
       "3                             17d ago                   4.0  \n",
       "4                             21d ago                   3.8  \n",
       "5                            1mon ago                   4.1  \n",
       "6                              1d ago                   3.5  \n",
       "7                             10d ago                   3.6  \n",
       "8                            1mon ago                   4.2  \n",
       "9                            1mon ago                   4.2  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambition_box = pd.DataFrame({})\n",
    "ambition_box['Company Name'] = company_names\n",
    "ambition_box['No. of days ago when job was posted'] = days_ago_posted\n",
    "ambition_box['Rating of the company'] = company_rating\n",
    "\n",
    "ambition_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49923c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3621d57",
   "metadata": {},
   "source": [
    "### 10. Python program to scrape the salary data for Data Scientist designation from http://www.ambitionbox.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1052fa6",
   "metadata": {},
   "source": [
    "Details to be scraped\n",
    "\n",
    "1. Company Name\n",
    "2. Number of salaries\n",
    "3. Average Salary\n",
    "4. Min salary\n",
    "5. Max Salary\n",
    "6. Experience required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6369f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a1e82d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "da9c003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the website\n",
    "url = 'http://www.ambitionbox.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5c7ad6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Category - Salary\n",
    "salary_category = driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[4]\")\n",
    "salary_category.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "edbe06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search input and search button\n",
    "\n",
    "search_input_salary = driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input\")\n",
    "search_input_salary.send_keys(\"Data Scientist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "65f89427",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_data_scientist = driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div\")\n",
    "select_data_scientist.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec1183",
   "metadata": {},
   "source": [
    "#### Extracting the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07e7c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Extracting company names\n",
    "\n",
    "company_names = []\n",
    "company_names_tags = driver.find_elements_by_xpath(\"//div[@class='company-info']/div/a\")\n",
    "for com in company_names_tags:\n",
    "    company = com.text\n",
    "    company_names.append(company)\n",
    "print(len(company_names))\n",
    "\n",
    "#Extracting Total Salary Record\n",
    "\n",
    "total_salary = []\n",
    "total_salary_tags = driver.find_elements_by_xpath(\"//div[@class='company-info']/div/span[1]\")\n",
    "for tot in total_salary_tags:\n",
    "    total = tot.text\n",
    "    total_salary.append(total)\n",
    "total_salary = [total_salary[x] for x in range(0,len(total_salary),2)]\n",
    "print(len(total_salary))\n",
    "\n",
    "#Extracting average salary\n",
    "average_salary = []\n",
    "avg_salary_tags = driver.find_elements_by_xpath(\"//div[@class='average-indicator-wrapper']/p\")\n",
    "for avg in avg_salary_tags:\n",
    "    average = avg.text\n",
    "    average_salary.append(average)\n",
    "print(len(average_salary))\n",
    "\n",
    "#Extracting minimum salary\n",
    "min_salary = []\n",
    "min_salary_tags = driver.find_elements_by_xpath(\"//div[@class='salary-values']/div[1]\")\n",
    "for min in min_salary_tags:\n",
    "    minimum = min.text\n",
    "    min_salary.append(minimum)\n",
    "print(len(min_salary))\n",
    "\n",
    "#Extracting maximum salary\n",
    "max_salary = []\n",
    "max_salary_tags = driver.find_elements_by_xpath(\"//div[@class='salary-values']/div[2]\")\n",
    "for max in max_salary_tags:\n",
    "    maximum = max.text\n",
    "    max_salary.append(maximum)\n",
    "print(len(max_salary))\n",
    "\n",
    "#Extracting experience required\n",
    "exp_required = []\n",
    "exp_required_tags = driver.find_elements_by_xpath(\"//div[@class='company-info-wrapper']/div/div[2]\")\n",
    "for exp in exp_required_tags:\n",
    "    experience = exp.text.replace('Data Scientist\\n . \\n','')\n",
    "    exp_required.append(experience)\n",
    "print(len(exp_required))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23ea62",
   "metadata": {},
   "source": [
    "We have scraped all the required details from the website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ff8b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f83cc",
   "metadata": {},
   "source": [
    "#### Creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "33e90641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Total Salary Record</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 12 salaries</td>\n",
       "      <td>₹ 28.7L</td>\n",
       "      <td>₹ 17.7L</td>\n",
       "      <td>₹ 35.0L</td>\n",
       "      <td>3 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 29 salaries</td>\n",
       "      <td>₹ 20.4L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 25.5L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliance Jio</td>\n",
       "      <td>based on 10 salaries</td>\n",
       "      <td>₹ 18.9L</td>\n",
       "      <td>₹ 5.6L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS</td>\n",
       "      <td>based on 15 salaries</td>\n",
       "      <td>₹ 15.9L</td>\n",
       "      <td>₹ 9.8L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>2 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optum</td>\n",
       "      <td>based on 27 salaries</td>\n",
       "      <td>₹ 15.3L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.4L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 81 salaries</td>\n",
       "      <td>₹ 15.1L</td>\n",
       "      <td>₹ 9.5L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>based on 45 salaries</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>based on 52 salaries</td>\n",
       "      <td>₹ 13.9L</td>\n",
       "      <td>₹ 8.3L</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Verizon</td>\n",
       "      <td>based on 14 salaries</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 21.0L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ganit Business Solutions</td>\n",
       "      <td>based on 13 salaries</td>\n",
       "      <td>₹ 12.4L</td>\n",
       "      <td>₹ 8.5L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Company Name   Total Salary Record Average Salary  \\\n",
       "0                   Walmart  based on 12 salaries        ₹ 28.7L   \n",
       "1                  Ab Inbev  based on 29 salaries        ₹ 20.4L   \n",
       "2              Reliance Jio  based on 10 salaries        ₹ 18.9L   \n",
       "3                        ZS  based on 15 salaries        ₹ 15.9L   \n",
       "4                     Optum  based on 27 salaries        ₹ 15.3L   \n",
       "5         Fractal Analytics  based on 81 salaries        ₹ 15.1L   \n",
       "6           Tiger Analytics  based on 45 salaries        ₹ 14.7L   \n",
       "7              UnitedHealth  based on 52 salaries        ₹ 13.9L   \n",
       "8                   Verizon  based on 14 salaries        ₹ 12.7L   \n",
       "9  Ganit Business Solutions  based on 13 salaries        ₹ 12.4L   \n",
       "\n",
       "  Minimum Salary Maximum Salary Experience Required  \n",
       "0        ₹ 17.7L        ₹ 35.0L           3 yrs exp  \n",
       "1        ₹ 15.0L        ₹ 25.5L         3-4 yrs exp  \n",
       "2         ₹ 5.6L        ₹ 26.2L           4 yrs exp  \n",
       "3         ₹ 9.8L        ₹ 20.0L           2 yrs exp  \n",
       "4        ₹ 11.0L        ₹ 22.4L         3-4 yrs exp  \n",
       "5         ₹ 9.5L        ₹ 22.0L         2-4 yrs exp  \n",
       "6         ₹ 9.0L        ₹ 20.0L         2-4 yrs exp  \n",
       "7         ₹ 8.3L        ₹ 20.5L         2-4 yrs exp  \n",
       "8        ₹ 10.0L        ₹ 21.0L           4 yrs exp  \n",
       "9         ₹ 8.5L        ₹ 15.0L           4 yrs exp  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_salary = pd.DataFrame({})\n",
    "\n",
    "ds_salary['Company Name'] = company_names\n",
    "ds_salary['Total Salary Record'] = total_salary\n",
    "ds_salary['Average Salary'] = average_salary\n",
    "ds_salary['Minimum Salary'] = min_salary\n",
    "ds_salary['Maximum Salary'] = max_salary\n",
    "ds_salary['Experience Required'] = exp_required\n",
    "\n",
    "ds_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455babb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
